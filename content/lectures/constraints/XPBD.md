---
title: 4. XPBD и SmallSteps
author: Роман Козак
type: docs
math: true
toc: true
sidebar:
  open: true
---

{{< add_script "js/constraints/pbd_cloth.js" >}}



## Интро
Здесь я расскажу сразу про две статьи. 
XPBD - Extended Position Based Dynamics написана в 2016 году. Это улучшение метода PBD. 
SmallSteps - написана в 2019 году. Скорее не улучшение XPBD, а режим работы который работает лучше всего. 

## Идея
Большая проблема PBD - большое влияние количества итераций релаксации на результат симуляции. 
В прошлой статье мы видели, как сильно отличается результат симуляции мягкого тела с 10 и 100 итерациями. 
Чем больше итераций тем жестче тело.
Вот так было в PBD:
{{< include_sketch path="constraints/sketch/pbd_cloth_sketch.js" base_name="pbd_cloth_sketch" >}}

XPBD предлагает решение этой проблемы. 

## XPBD

В статье улучшаются мягкие ограничения. Это ограничения, которые ведут себя как критически демпфированные пружины. 
Они постепенно сходятся к желаемому значению. 
Сложность в том, что они должны себя вести примерно одинаково вне зависимости от количества итераций. 
Для жестких ограничений ничего не поменялось. 

Дальше пойдет математика, которая не нужна для применения. 
Кому нужна конечная формула смело пропускайте следующий раздел

### Математика XPBD
Базовая схема такая же как и в PBD. Сначала расчитываем позицию точки $x$ как будто она двигается без ограничений. 
Потом применяем ограничения.

#### Получаем уравнения
Ограничения действуют силами $f$. У этих ограничений есть потенциальная энергия $U$.

$$
f = -\frac{\partial U(x)}{\partial x}
$$

$$
M x'' = f(x)
$$
M -- это матрица массы.

$$
M x'' = -\frac{\partial U(x)}{\partial x}
$$

Теперь используют неявную численную схему:
$$
\begin{equation}
M \frac{x^{n+1} - 2x^n + x^{n-1}}{{\Delta t}^2} = -\frac{\partial U(x^{n+1})}{\partial x}
\end{equation}
$$
Эта схема неявная, так как $x^{n+1}$ зависит от U, которое зависит от $x^{n+1}$.
Для почти всех систем неявная схема абсолютно устойчива.

#### Вводим потенциал

Теперь нужно ввести $U$ для ограничений.
Мы считаем что уравнение ограничения $C(x) = 0$. зависит только от $x$. А еще мы считаем, что оно дифференцируемо.
Вообще можно в качестве потенциала использовать любую функцию, но нам полезно добиться хороших свойств от него.
- $U(x) = 0$ если $C(x) = 0$
Т.е когда уравнение ограничения выполняется, потенциал равен 0 и сира ограничения тоже равна нулю. Что логично.
- При отклонении от $C(x) = 0$ потенциал растет. Ну и сила ограничения тоже будет расти.

Для этого хорошо подходит квадратичный потенциал:
$$
U(x) = \frac{1}{2} С(x)^T * \alpha^{-1} * C(x)
$$
Здесь $C(x)$ это вектор ограничений, а $\alpha$ это матрица compliance. 
Вводится термин compliance $\alpha$: Это параметр обратный жесткости. Оно в пределах $(0; +\inf)$ Чем он больше тем мягче ограничение.
Эта матрица может быть не только диагональной. Благодаря этому можно комбинаю жесткости разных ограничений.

{{<details title="Пример" closed="true">}}
У нас есть простое уравнение пружинки:
$$
C(x) = x = 0
$$
И жесткость пружинки $k = \alpha^{-1}$.
$$
U(x) = \frac{1}{2} (x^T * \alpha^{-1} * x) = \frac{1}{2} k x^2
$$

Тогда сила ограничения будет равна:
$$
f = -\frac{\partial U(x)}{\partial x} = - \nabla C(x) * \alpha^{-1} * C(x)
$$
{{< /details >}}

Дальше вводим переменную $\lambda$ которая будет множителем Лагранжа.
$$
\lambda = -\tilde{\alpha} * C(x)
$$
Это уравнение можно переписать вот так:
$$
C(x) + \tilde{\alpha} * \lambda = 0
$$

Тут $\tilde{\alpha}$ это $\frac{\alpha} {\Delta t^2}$. 

#### Уравнения которые нужно решить

Подставляем уравнение силы и $\lambda$ в уравнение движения (1) и получаем финальную систему, которую нужно решить:
$$
\begin{equation}
\begin{aligned}
M(x^{n+1} - \tilde{x}) - \Delta t^2 \nabla C(x^{n+1}) *\lambda^{n+1} = 0 \\\
C(x^{n+1}) + \tilde{\alpha} * \lambda^{n+1} = 0
\end{aligned}
\end{equation}
$$
Где $\tilde{x} = x^n + \Delta t * v^n$

#### Решаем

Получили нашу любимую нелинейную систему уравнений. Нам нужно найти  $x^{n+1}$ и $\lambda^{n+1}$. В общем виде ее решить невозможно.
Поэтому в статье предлагают решать ее итерационным методом Ньютона.
Обозначим уравнения из (2) как

$$
\begin{aligned}
g(x^{n+1}, \lambda^{n+1}) = 0 \\\
h(x^{n+1}, \lambda^{n+1}) = 0
\end{aligned}
$$

Линеаризовываем их относительно $x^{n+1}$ и $\lambda^{n+1}$:

Раньше мы писали ${n+1}$, и это означало значение на следующем шаге. Мы сейчас будем искать $x^{n+1}$ и $\lambda^{n+1}$ итеративно. И поэтому будем писать $x^{i}$ и $\lambda^{i}$, где $i$ -- номер итерации. Т.е. $x^{i}$ это приближение к $x^{n+1}$ на $i$-й итерации.

$$
\begin{bmatrix}
K,  &\nabla C^{T}(x^{i}) \\\
\nabla C{(x^{i})}, &\tilde{\alpha}
\end{bmatrix}
\begin{bmatrix}
\Delta x^{i} \\\
\Delta \lambda^{i}
\end{bmatrix} = 
\begin{bmatrix}
g(x^{i}, \lambda^{i}) \\\
h(x^{i}, \lambda^{i})
\end{bmatrix}
$$
Где 

$$
\begin{aligned}
&K = \partial{g(x^{n+1}, \lambda^{n+1})} / \partial{x^{n+1}}  = \\\
&= M - \Delta t^2 \nabla C(x^{n+1}) \nabla C^{T}(x^{n+1}) \\\
\end{aligned}
$$

Из этого уравнения мы получаем $\Delta x^{i}$ и $\Delta \lambda^{i}$, которые мы используем для улучшения наших приближений:
$$
\begin{aligned}
x^{i+1} = x^{i} + \Delta x^{i} \\\
\lambda^{i+1} = \lambda^{i} + \Delta \lambda^{i}
\end{aligned}
$$

И так делаем пока не сойдемся.
В качестве начального приближения $x^{0} = \tilde{x}$ и $\lambda^{0} = 0$.

Проблема этого метода в том, что его не всегда просто реализовать. Нужно расчитывать матрицу $K$
 у которой второе слагаемое -- Гессиан функции $C(x)$. Это сложно.

#### Упрощаем

- Гессиан ограничений дает поправку как $O(\Delta t^2)$. Это мало. Мы можем его просто выкинуть. Тогда $K = M$. Это константная матрица масс, которая известна с самого начала.
- $g(x^{i}, \lambda^{i}) = 0$. 
Для начальных условий $g(x^{0}, \lambda^{0}) = 0$. 
Остальные итерации изменяют $g$ на малую величину, которая зависит от изменения градиента ограничений. Если ограничение линейное, то $g(x^{i}, \lambda^{i}) = 0$ всегда. 

И вот мы получаем простую систему уравнений:
$$
\begin{bmatrix}
M, &\nabla C^{T}(x^{i}) \\\
\nabla C{(x^{i})}, &\tilde{\alpha}
\end{bmatrix}
\begin{bmatrix}
\Delta x^{i} \\\
\Delta \lambda^{i}
\end{bmatrix} = 
\begin{bmatrix}
0 \\\
h(x^{i}, \lambda^{i})
\end{bmatrix}
$$

Переписав уравнения в удобной форме получаем:
$$
\begin{equation}
\begin{bmatrix}
\nabla C(x^{i}) M^{-1} \nabla C^{T}(x^{i}) + \tilde{\alpha}
\end{bmatrix}
\Delta \lambda = -C(x^{i}) - \tilde{\alpha} \lambda^{i}
\end{equation}
$$

$$
\begin{equation}
\Delta x = M^{-1} \nabla C^{T}(x^{i}) \Delta \lambda
\end{equation}
$$

### Конец уже близок

В уравнении (3) мы имеем систему линейных уравнений относительно $\Delta \lambda$. Последнее что нужно сделать это решить ее.
Для этого можно использовать  Gauss-Seidel итерации, который уде использовали в PBD. 
Если у нас матрица complience диагональная, то каждую  $\Delta \lambda$ можно  искать вот так:

$$
\begin{equation}
\Delta \lambda_{j} = -\frac{C_{j}(x^{i}) - \tilde{\alpha_{j}} \lambda_{j}^{i}}{\nabla C_{j} M^{-1} \nabla C_{j}^{T} + \tilde{\alpha_{j}}}
\end{equation}
$$

И вот наконец-то мы приходим к финальной схеме:
Расчитываем $\Delta \lambda$ по формуле (5), а потом по формуле (4) находим $\Delta x$ И смещаем нужные точки. 
И так для каждого ограничения. И повторяем нужное количество итераций. 
Что самое крутое, так это то, что для абсолютно жесткого ограничения где $\alpha = 0$ мы получаем PBD если занулим $\alpha$ в формуле (5)

Еще из приятностей. У нас есть \lambda,  с помощью которой можно найти силу ограничения. 
И это позволяет делать ограничения, которые можно отключать если силы ограничения становятся слишком большими.


## SmallSteps


## Источники
- Оригинальная статья [XPBD](https://matthias-research.github.io/pages/publications/XPBD.pdf)
- [Статья](https://ep.liu.se/ecp/019/005/ecp01905.pdf) в которой рассказывают почему квадратичный потенциал хорошо подходит для ограничений
- [Статья](https://www.cs.columbia.edu/cg/pdfs/131-ESIC.pdf) в которой рассказывают почему можно обнулять g(x^{i}, \lambda^{i}) и как это влияет на результат